{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup\n",
        "\n",
        "First, we import all the necessary libraries and then read in the csv files. We clean the data by removing all the rows of our accessibility dataset with na entries."
      ],
      "metadata": {
        "id": "UT7BTyUSD1c6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import requests\n",
        "from pathlib import Path\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "from shapely.geometry import Point"
      ],
      "metadata": {
        "id": "5890-MFMiUP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/Access_to_Everyday_Life_Dataset.csv\")\n",
        "df = df.dropna()\n",
        "points_gdf = gpd.GeoDataFrame(\n",
        "    df,\n",
        "    geometry=gpd.points_from_xy(\n",
        "        df[\"geometry/coordinates/0\"],\n",
        "        df[\"geometry/coordinates/1\"]\n",
        "    ),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "block_groups = gpd.read_file(\n",
        "    \"/content/CensusBGGEO_2020_-3103699184132166589.geojson\"\n",
        ")"
      ],
      "metadata": {
        "id": "8cXDmFfAicsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we join the accessibility data with Seattle data sets, grouping by census block groups."
      ],
      "metadata": {
        "id": "H_lHUWg7i-6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "points_with_bg = gpd.sjoin(\n",
        "    points_gdf,\n",
        "    block_groups[[\"GEOID_20\", \"geometry\"]],\n",
        "    how=\"left\",\n",
        "    predicate=\"within\"\n",
        ")\n",
        "points_with_bg['GEOID_20'] = pd.to_numeric(points_with_bg['GEOID_20'])\n",
        "\n",
        "bg_csv = pd.read_csv(\n",
        "    \"CensusBGGEO_2020_-8839192176228303427.csv\"\n",
        ")\n",
        "\n",
        "final = points_with_bg.merge(\n",
        "    bg_csv,\n",
        "    on=\"GEOID_20\",\n",
        "    how=\"left\"\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dQgVFt5nQglt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then decide select which attributes will be needed to train the model."
      ],
      "metadata": {
        "id": "7TjQ1MR-c1CM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attributes = [\n",
        "              # Tract and Block Group which we will group upon\n",
        "              'Census Tract and Block Group',\n",
        "              # Features\n",
        "              'Per Capita Income',\n",
        "              'Median Age',\n",
        "              'No Vehicles Available',\n",
        "              \"Population 18 years and Over with a Disability\",\n",
        "              \"Land Acres\",\n",
        "              \"Natural resources, construction, and maintenance occupations\",\n",
        "              \"Population 20 to 64 years for whom poverty status is determined\",\n",
        "              \"Total Population\",\n",
        "              \"Bachelor degree or higher\",\n",
        "              \"Families with income in the past 12 months below poverty level\",\n",
        "              \"Families for whom poverty status is determined\",\n",
        "              # Labels\n",
        "              'properties/severity']"
      ],
      "metadata": {
        "id": "MlVsINtTTXxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_data = final.groupby(attributes[:-1], as_index=False)['properties/severity'].mean()"
      ],
      "metadata": {
        "id": "-3xWk0-WUb8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create Model and Train\n"
      ],
      "metadata": {
        "id": "U3l_OvpIdhe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(in_features, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, out_features)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return 1 + 4 * torch.sigmoid(self.layers(x))"
      ],
      "metadata": {
        "id": "sFRkizXre-Su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the test and train sets."
      ],
      "metadata": {
        "id": "ufnZxqaEd35n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_np = group_data[attributes[1:-1]].to_numpy().astype(np.float32)\n",
        "y_np = group_data[\"properties/severity\"].to_numpy().astype(np.float32).reshape(-1, 1)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X_np, y_np, test_size=0.2, random_state=20\n",
        ")"
      ],
      "metadata": {
        "id": "NvHir2FPkN-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we standardize the data."
      ],
      "metadata": {
        "id": "VD2sY7spd-Sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = X_train_np.mean(axis=0, keepdims=True)\n",
        "std  = X_train_np.std(axis=0, keepdims=True) + 1e-8\n",
        "X_train_np = (X_train_np - mean) / std\n",
        "X_test_np  = (X_test_np  - mean) / std\n",
        "X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
        "X_test  = torch.tensor(X_test_np, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train_np, dtype=torch.float32)\n",
        "y_test  = torch.tensor(y_test_np, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "7sPuO6EAkOox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate the model."
      ],
      "metadata": {
        "id": "h63p93CLfnFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression(\n",
        "    in_features=len(attributes) - 2,\n",
        "    out_features=1\n",
        ")"
      ],
      "metadata": {
        "id": "Uw4KzF7XkSvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create the loss and optimizer functions. Then, we define the accuracy function."
      ],
      "metadata": {
        "id": "xelVZITNeKOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)"
      ],
      "metadata": {
        "id": "0aEht5v8kWzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "  correct = torch.eq(y_true, y_pred).sum().item()\n",
        "  acc = (correct / len(y_true)) * 100\n",
        "  return acc;"
      ],
      "metadata": {
        "id": "S1Xq21ywz3FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then, train the model."
      ],
      "metadata": {
        "id": "2Sy_dC0ifHtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3000 #6000 at this point\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    predictions = model(X_train)\n",
        "    loss = criterion(predictions, y_train)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch} | Loss: {loss.item():.4f} | Acc: {accuracy(torch.round(y_train), torch.round(predictions)):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNBZxqqnkYvH",
        "outputId": "9c22294b-31a9-433a-9f2d-ab12ef6a15de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 0.5883 | Acc: 42.0804\n",
            "Epoch 100 | Loss: 0.5386 | Acc: 42.0804\n",
            "Epoch 200 | Loss: 0.5007 | Acc: 43.0260\n",
            "Epoch 300 | Loss: 0.4703 | Acc: 45.1537\n",
            "Epoch 400 | Loss: 0.4449 | Acc: 48.4634\n",
            "Epoch 500 | Loss: 0.4237 | Acc: 49.4090\n",
            "Epoch 600 | Loss: 0.4061 | Acc: 50.3546\n",
            "Epoch 700 | Loss: 0.3917 | Acc: 52.7187\n",
            "Epoch 800 | Loss: 0.3802 | Acc: 52.7187\n",
            "Epoch 900 | Loss: 0.3710 | Acc: 53.9007\n",
            "Epoch 1000 | Loss: 0.3637 | Acc: 55.0827\n",
            "Epoch 1100 | Loss: 0.3576 | Acc: 55.7920\n",
            "Epoch 1200 | Loss: 0.3523 | Acc: 55.0827\n",
            "Epoch 1300 | Loss: 0.3476 | Acc: 54.8463\n",
            "Epoch 1400 | Loss: 0.3433 | Acc: 55.0827\n",
            "Epoch 1500 | Loss: 0.3393 | Acc: 55.0827\n",
            "Epoch 1600 | Loss: 0.3355 | Acc: 54.6099\n",
            "Epoch 1700 | Loss: 0.3318 | Acc: 54.8463\n",
            "Epoch 1800 | Loss: 0.3282 | Acc: 55.0827\n",
            "Epoch 1900 | Loss: 0.3246 | Acc: 55.0827\n",
            "Epoch 2000 | Loss: 0.3211 | Acc: 55.0827\n",
            "Epoch 2100 | Loss: 0.3176 | Acc: 54.8463\n",
            "Epoch 2200 | Loss: 0.3142 | Acc: 55.0827\n",
            "Epoch 2300 | Loss: 0.3108 | Acc: 54.6099\n",
            "Epoch 2400 | Loss: 0.3074 | Acc: 54.8463\n",
            "Epoch 2500 | Loss: 0.3041 | Acc: 54.8463\n",
            "Epoch 2600 | Loss: 0.3007 | Acc: 54.6099\n",
            "Epoch 2700 | Loss: 0.2974 | Acc: 54.8463\n",
            "Epoch 2800 | Loss: 0.2941 | Acc: 55.3191\n",
            "Epoch 2900 | Loss: 0.2908 | Acc: 55.5556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we get the accuracy from the test dataset."
      ],
      "metadata": {
        "id": "gD-OH4hAfOkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    prediction = torch.round(model(X_test))\n",
        "\n",
        "\n",
        "accuracy(torch.round(y_test), prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3lIgN9okdpb",
        "outputId": "a4faa849-6249-4b13-b220-127637834146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59.43396226415094"
            ]
          },
          "metadata": {},
          "execution_count": 474
        }
      ]
    }
  ]
}